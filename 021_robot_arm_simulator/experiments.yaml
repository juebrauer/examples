# ============================================================================
# Experiment Configuration for Robot Arm Controller Comparison
# ============================================================================
#
# Usage:
#   python robot_arm_controller.py --config experiments.yaml              # run all
#   python robot_arm_controller.py --config experiments.yaml --only 1     # run experiment 1
#   python robot_arm_controller.py --config experiments.yaml --only 1,3,5 # run experiments 1, 3, 5
#
# Result folders are created automatically:
#   results/experiment001_cnn_data_2dof_100000/
#   results/experiment002_resnet_scratch_data_2dof_100000/
#   ...
# Each folder contains: training.log, learning_curves.png, final_model.pt

# ---------------------------------------------------------------------------
# Shared defaults (applied to all experiments unless overridden)
# ---------------------------------------------------------------------------
defaults:
  data_dir: ./data_2dof_100000       # folder with samples.csv + images/
  results_dir: ./results             # base directory for all experiment outputs

  train_split: 0.8                   # 80% train, 20% test
  epochs: 20
  batch_size: 64
  lr: 1e-3
  min_lr: 1e-6
  weight_decay: 1e-4
  warmup_epochs: 0
  scheduler: cosine                  # "none" or "cosine"
  label_smoothing: 0.0
  image_size: 224
  num_workers: 2
  use_amp: false                     # mixed precision (CUDA only)

  augmentation:
    enabled: true
    rotate_deg: 10.0
    translate: 0.05
    scale: 0.10
    brightness: 0.15
    contrast: 0.15
    erasing_p: 0.0

  # VLA-specific defaults (only used by VLA experiments)
  vla:
    base_model: "unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit"
    load_in_4bit: true
    max_seq_length: 2048
    lora_r: 16
    lora_alpha: 16
    lora_dropout: 0.0
    finetune_vision_layers: true
    finetune_language_layers: true
    finetune_attention_modules: true
    finetune_mlp_modules: true
    train_batch_size: 2
    gradient_accumulation_steps: 4
    lr: 2e-4
    weight_decay: 1e-3
    warmup_steps: 5
    epochs: 2
    optimizer: adamw_8bit
    lr_scheduler: linear
    seed: 3407
    max_new_tokens: 16
    temperature: 0.2

# ---------------------------------------------------------------------------
# Experiments
# ---------------------------------------------------------------------------
experiments:
  - name: cnn
    model: cnn
    # uses all defaults

  - name: resnet_scratch
    model: resnet
    pretrained: false

  - name: resnet_pretrained
    model: resnet
    pretrained: true

  - name: vit_scratch
    model: vit
    pretrained: false
    epochs: 10                       # ViT from scratch may need different tuning
    lr: 5e-4

  - name: vit_pretrained
    model: vit
    pretrained: true
    epochs: 10
    lr: 1e-4                         # lower LR for finetuning

  - name: vla
    model: vla
    # uses vla defaults from above
