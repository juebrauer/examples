# ============================================================================
# Experiment Configuration for Robot Arm Controller Comparison
# ============================================================================
#
# Usage:
#   python robot_arm_controller.py --config experiments.yaml
#
# Toggle experiments on/off with the "enabled" flag.
#
# Result folders are created automatically:
#   results/experiment001_cnn_data_2dof_100000/
#   results/experiment002_resnet_scratch_data_2dof_100000/
#   ...
# Each folder contains: training.log, learning_curves.png, final_model.pt
# A combined results.txt summary table is written to the results directory.

# ---------------------------------------------------------------------------
# Shared defaults (applied to all experiments unless overridden)
# ---------------------------------------------------------------------------
defaults:
  quicktest: false                   # true = 1 epoch, 1% train, 1% test (smoke test)

  data_dir: ./data_dof2_100000       # folder with samples.csv + images/
  results_dir: ./results             # base directory for all experiment outputs

  train_split: 0.8                   # 80% train, 20% test
  train_test_subsample: 0.25         # fraction of train AND test data actually used (1.0 = all)
  epochs: 50
  batch_size: 64
  lr: 1e-3
  min_lr: 1e-6
  weight_decay: 1e-4
  warmup_epochs: 0
  scheduler: cosine                  # "none" or "cosine"
  label_smoothing: 0.0
  image_size: 224
  num_workers: 2
  use_amp: false                     # mixed precision (CUDA only)

  augmentation:
    enabled: true
    rotate_deg: 10.0
    translate: 0.05
    scale: 0.10
    brightness: 0.15
    contrast: 0.15
    erasing_p: 0.0

  # VLA-specific defaults (only used by VLA experiments)
  vla:
    base_model: "unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit"
    load_in_4bit: true
    max_seq_length: 2048
    lora_r: 16
    lora_alpha: 16
    lora_dropout: 0.0
    finetune_vision_layers: true
    finetune_language_layers: true
    finetune_attention_modules: true
    finetune_mlp_modules: true
    train_batch_size: 2
    gradient_accumulation_steps: 4
    lr: 2e-4
    weight_decay: 1e-3
    warmup_steps: 5
    epochs: 2
    optimizer: adamw_8bit
    lr_scheduler: linear
    seed: 3407
    max_new_tokens: 16
    temperature: 0.2

# ---------------------------------------------------------------------------
# Experiments â€“ set "enabled: false" to skip an experiment
# ---------------------------------------------------------------------------
experiments:
  - name: cnn
    enabled: false
    model: cnn
    epochs: 20

  - name: resnet_scratch
    enabled: false
    model: resnet
    pretrained: false
    epochs: 20

  - name: resnet_pretrained
    enabled: false
    model: resnet
    pretrained: true
    epochs: 20

  - name: vit_scratch
    enabled: true
    model: vit
    pretrained: false
    epochs: 50
    lr: 1e-4                  
    warmup_epochs: 10
    train_test_subsample: 0.25

  - name: vit_pretrained
    enabled: true
    model: vit
    pretrained: true
    epochs: 20
    lr: 1e-4
    train_test_subsample: 0.25

  - name: vla
    enabled: true             
    model: vla
    epochs: 5                 
    train_test_subsample: 0.05